We found the github page "Image Similarity using Deep Ranking" (https://github.com/akarshzingade/image-similarity-deep-ranking). On a high level the process work like this. We use a neural network to map images to a high degree space. In this space we can measure the distance by simply calculating the euclidean distance between two images.
Let f(I) be the nn applied to the images I and let D(x,y) be the euclidean distance between x and y. A triplet consists of query image, positive image and negative image we will shorten to (q, p, n). ImageDataGeneratorCustom (IDGC) loads triplets into memory in order as they are needed for training. Further more the IDGC also randomly applies transformations like rotate, zoom and shear to the image. These transformations help our predicts to be independent to rotation, angle and framing. The image order is important because the loss function uses three consecutive images to calculate a hingeloss like loss.

triplet_loss: loss(q, p, n) = max(0, 1 + D(f(q), f(p)) - D(f(q), f(n)))
average loss over batch

Now we have a model we can train but even with a trained model we only have a mapping form a image to am abstract taste space. To decide if a given test triplet is valid we have to calculate the point in the taste space for the triplet. Given these three points we can calculate the distances D_q_p = D(f(q), f(p) and  D_q_n = D(f(q), f(n)) if D_q_p < D_q_p we can output 1 otherwise we output 0. Later we added caching of previously calculated coordinates and distances to speed up the prediction process. The first results of the deepranking model were promising but we were no able to improve the model. Next we switched the model to the pretrained InceptionV3 but kept the loss and IDGC the same. We added dropout and two dense layer to the model and set all but our added layers and the top layer to be not trainable. After experimenting with the dropout parameter we found 0.5 to give us the best scores.
